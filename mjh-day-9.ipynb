{"cells":[{"cell_type":"markdown","metadata":{"id":"HKmorPdno_n_"},"source":["# MMAI 2025 869: Team Project Template\n","*Updated May 3, 2024*\n","\n","This notebook serves as a template for the Team Project. Teams can use this notebook as a starting point, and update it successively with new ideas and techniques to improve their model results.\n","\n","Note that is not required to use this template. Teams may also alter this template in any way they see fit.\n"]},{"cell_type":"markdown","metadata":{"id":"oZFTCX4DqmRO"},"source":["# Preliminaries: Inspect and Set up environment\n","\n","No action is required on your part in this section. These cells print out helpful information about the environment, just in case."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xj34Jz-Do_oK"},"outputs":[],"source":["import datetime\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqQ_XOKyXTS6"},"outputs":[],"source":["print(datetime.datetime.now())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfOMt1lErLhZ"},"outputs":[],"source":["!which python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aub2w1-arM5K"},"outputs":[],"source":["!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9Y_n_8UrO9i"},"outputs":[],"source":["!echo $PYTHONPATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qyD7Jl0Gw1E"},"outputs":[],"source":["! pip install --user xgboost\n","! pip install --user fancyimpute\n","! pip install --user sklearn-genetic\n","! pip install --user ipywidgets"]},{"cell_type":"markdown","metadata":{"id":"B_IHoz7f2yIV"},"source":["# 0. Data Loading and Inspection"]},{"cell_type":"markdown","metadata":{"id":"jqm_REd4oouz"},"source":["## 0.1: Load data\n","\n","The file containing the labeled training data is conveniently located on the cloud at the address below. Let's load it up and take a look."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6b_BM0Nz9sF"},"outputs":[],"source":["df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1eYCKuqJda4bpzXBVnqXylg0qQwvpUuum\")\n","target_feature = 'h1n1_vaccine'"]},{"cell_type":"markdown","metadata":{"id":"Ys9PPIOlvVzl"},"source":["## 0.1 Simple Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5sQ8ht2_L0cD"},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxX8nM24uyzE"},"outputs":[],"source":["# Let's print some descriptive statistics for all the numeric features.\n","\n","df.describe().T\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELhSsAmiLZxF"},"outputs":[],"source":["# What is the number of unique values in all the categorical features? And what is\n","# the value with the highest frequency?\n","\n","df.describe(include=object).T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBI28r_bgV06"},"outputs":[],"source":["# How much missing data is in each feature?\n","\n","df.isna().sum()"]},{"cell_type":"markdown","metadata":{},"source":["### Taking a look at the class imbalance of our target"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","from matplotlib import pyplot as plt\n","from sklearn.preprocessing import LabelEncoder,OrdinalEncoder\n","\n","def get_target_skew_rate(data_target):\n","    target_df = pd.DataFrame(data_target)\n","    sns.countplot(x=target_feature, data=target_df)\n","\n","    no_vaccine_count = len(target_df[target_df[target_feature]==0])\n","    yes_vaccine_count = len(target_df[target_df[target_feature]==1])\n","    print(f\"No vaccine: {no_vaccine_count}\")\n","    print(f\"Has vaccine: {yes_vaccine_count}\")\n","\n","    # save this for later...\n","    training_data_pos_scale_weight = (no_vaccine_count / yes_vaccine_count)\n","    print(f\"training_data_pos_scale_weight: {training_data_pos_scale_weight}\")\n","    return training_data_pos_scale_weight\n","\n","training_data_pos_scale_weight = get_target_skew_rate(df[target_feature])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AC8gMKmad5RF"},"outputs":[],"source":["# For convienience, let's save the names of all numeric features to a list,\n","# and the names of all categorical features to another list.\n","\n","numeric_features = [\n","          \"h1n1_concern\",\n","          \"h1n1_knowledge\",\n","          \"behavioral_antiviral_meds\",\n","          \"behavioral_avoidance\",\n","          \"behavioral_face_mask\",\n","          \"behavioral_wash_hands\",\n","          \"behavioral_large_gatherings\",\n","          \"behavioral_outside_home\",\n","          \"behavioral_touch_face\",\n","          \"doctor_recc_h1n1\",\n","          \"doctor_recc_seasonal\",\n","          \"chronic_med_condition\",\n","          \"child_under_6_months\",\n","          \"health_worker\",\n","          \"health_insurance\",\n","          \"opinion_h1n1_vacc_effective\",\n","          \"opinion_h1n1_risk\",\n","          \"opinion_h1n1_sick_from_vacc\",\n","          \"opinion_seas_vacc_effective\",\n","          \"opinion_seas_risk\",\n","          \"opinion_seas_sick_from_vacc\",\n","          \"household_adults\",\n","          \"household_children\",\n","]\n","\n","behavioural_features = list(x for x in numeric_features if 'behavioral' in x)\n","\n","categorical_features = [\n","    \"age_group\",\n","    \"education\",\n","    \"race\",\n","    \"sex\",\n","    \"income_poverty\",\n","    \"marital_status\",\n","    \"rent_or_own\",\n","    \"employment_status\",\n","    \"hhs_geo_region\",\n","    \"census_msa\",\n","    \"employment_industry\",\n","    \"employment_occupation\",\n","]\n","\n","all_features = numeric_features + categorical_features\n","\n","all_cat_features = categorical_features\n","\n","continuous_features = [\n","    'opinion_h1n1_vacc_effective',\n","    'opinion_h1n1_risk',\n","    'opinion_h1n1_sick_from_vacc',\n","    'opinion_seas_vacc_effective',\n","    'opinion_seas_risk',\n","    'opinion_seas_sick_from_vacc',\n","]"]},{"cell_type":"markdown","metadata":{},"source":["### helper method for getting the least correlated items to the target"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_low_correlations_for_target(data, show_plot = False):\n","    corr = data.corr()\n","\n","    threshold = 0.01\n","    tf_corr = corr[(abs(corr[target_feature]) <= threshold)]\n","    display(tf_corr)\n","\n","    if show_plot:\n","        g = sns.heatmap(corr,  vmax=.3, center=0,\n","                    square=True, linewidths=1, \n","                    cbar_kws={\"shrink\": .5}, annot=True, \n","                    fmt='.2f', cmap='coolwarm')\n","        sns.despine()\n","        g.figure.set_size_inches(30,25)\n","        plt.show()\n","\n","    return list(tf_corr.index)"]},{"cell_type":"markdown","metadata":{},"source":["## How do each of the behavioural features correlate to the target?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_bv_target = pd.concat((df[behavioural_features], df[target_feature]), axis=1)\n","df_bv_target.hist()\n","get_low_correlations_for_target(df_bv_target, show_plot=True)\n","\n","# separate these by skewedness in correlation with the target\n","# so when we FE combine these, it makes sense\n","behavioural_features_neg = [\n","          \"behavioral_avoidance\",\n","          \"behavioral_wash_hands\",\n","          \"behavioral_outside_home\",\n","          \"behavioral_touch_face\",\n","]\n","\n","behavioural_features_pos = [x for x in behavioural_features if x not in behavioural_features_neg]\n"]},{"cell_type":"markdown","metadata":{"id":"sbTJkUNdvfsF"},"source":["# 1. Train/Test Split\n","\n","Now we randomly split the available data into train and test subsets.\n","\n","The training data will later be used to build and assess the model on various combinations of hyperparaters.\n","\n","The testing data will be used as a \"final estimate\" of a model's performance."]},{"cell_type":"markdown","metadata":{"id":"sdiKKblCo53S"},"source":["# 2. Model 1 (A simple DecisionTree model)\n","\n","As a baseline, we'll do the absolute bare minimum data cleaning and then quickly build a simple Decision Tree."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuWoCrg3bQUs"},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import cross_validate\n","# from sklearn.model_selection import train_test_split\n","import xgboost as xgb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUZxa2f6uw3l"},"outputs":[],"source":["# Scikit-learn needs us to put the features in one dataframe, and the label in another.\n","# It's tradition to name these variables X and y, but it doesn't really matter.\n","\n","X = df.drop(target_feature, axis=1)\n","y = df[target_feature]\n","\n","# X[continuous_features].hist()\n","# X[behavioural_features].hist()"]},{"cell_type":"markdown","metadata":{"id":"zqYbbTAejRCD"},"source":["## 1.1 Cleaning and FE"]},{"cell_type":"markdown","metadata":{},"source":["### Feature Encoding / Label Mapping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KwfR9_nQftlS"},"outputs":[],"source":["label_mapping = {}\n","\n","def apply_label_mapping_fxn(row):\n","    global label_mapping\n","    for feature in all_cat_features:\n","        feature_mapping = label_mapping[feature]\n","        enc_value = feature_mapping[row[feature]]\n","        row[feature] = enc_value\n","    return row\n","\n","def apply_label_mapping(data):\n","    return data.apply(apply_label_mapping_fxn, axis=1)\n","\n","def update_row_mappings(row):\n","    global label_mapping\n","    for feature in all_cat_features:\n","        if not feature in label_mapping:\n","            label_mapping[feature] = {}\n","        enc_feature_name = f\"{feature}_enc\"\n","        orig_value = row[feature]\n","        enc_value = row[enc_feature_name]\n","        label_mapping[feature][orig_value] = enc_value\n","\n","    return row\n","\n","def set_label_mapping(orig_data, encoded_data):\n","    global label_mapping\n","    map_df = pd.DataFrame()\n","    for feature in all_cat_features:\n","        map_df[feature] = orig_data[feature]\n","        map_df[f\"{feature}_enc\"] = encoded_data[feature]\n","    map_df.apply(update_row_mappings, axis=1)\n","\n","def label_encoding(data):\n","    global labeled_columns\n","\n","    import category_encoders as ce\n","    encoder = ce.JamesSteinEncoder(cols=all_cat_features)\n","    labeled = encoder.fit_transform(data, y)\n","    labeled_columns = list(encoder.get_feature_names_out())\n","\n","    set_label_mapping(data, labeled)\n","\n","    return labeled\n"]},{"cell_type":"markdown","metadata":{},"source":["### Imputation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4l5INJbdOde"},"outputs":[],"source":["from fancyimpute import SoftImpute\n","\n","def impute_data(data):\n","    return pd.DataFrame(SoftImpute(verbose=False).fit_transform(data), columns=data.columns)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Scale our skewed features (using Box-Cox)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import PowerTransformer\n","display(X[continuous_features].describe().T)\n","\n","def set_non_zero(row):\n","    for feature in continuous_features:\n","        if row[feature] == 0.0:\n","            row[feature] = 0.00000001\n","    return row\n","\n","def set_continuous_features(data):\n","    # data[continuous_features] = data[continuous_features].apply(set_non_zero, axis=1)\n","\n","    display(data[continuous_features].describe().T)\n","\n","    scaler = PowerTransformer(method='yeo-johnson')\n","    scaler.fit(data[continuous_features])\n","    data[continuous_features] = scaler.transform(data[continuous_features])\n","    # data[continuous_features].hist()\n","    return data"]},{"cell_type":"markdown","metadata":{},"source":["### Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def feature_engineering(data):\n","    data['insured_family_size'] = data['household_adults'] + data['health_insurance'] + data['household_children']\n","    data['at_risk_patient'] = data['doctor_recc_h1n1'] + data['doctor_recc_seasonal'] + data['chronic_med_condition']\n","\n","    data['behavioral_risk_neg'] = data[behavioural_features_neg].sum(axis=1)\n","    data['behavioral_risk_pos'] = data[behavioural_features_pos].sum(axis=1)\n","    # data = data.drop(behavioural_features, axis=1)\n","\n","    data['opinion_seas_risk'] = data['opinion_seas_risk'] + data['opinion_seas_sick_from_vacc']\n","    data['opinion_h1n1'] = data['opinion_h1n1_vacc_effective'] + data['h1n1_concern']\n","    data['poverty_vs_insurance'] = data['income_poverty'] + data['health_insurance']\n","    return data\n"]},{"cell_type":"markdown","metadata":{},"source":["## Correlation mapping...\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["heatmap_data = pd.concat([label_encoding(X), y], axis=1)\n","low_correlation_features = get_low_correlations_for_target(heatmap_data)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Our primary data cleaning / preprocessing pipeline method"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def clean_data(data, use_fe=False):\n","    ret = data\n","    \n","    ret = apply_label_mapping(ret)\n","\n","    ret = impute_data(ret)\n","    ret = set_continuous_features(ret)\n","\n","    if use_fe:\n","        ret = feature_engineering(ret)\n","\n","    # ret = drop_not_used_ga_features(ret)\n","\n","    # use our full training set to get the correlation data\n","    # ret = ret.drop(low_correlation_features, axis=1)\n","\n","    return ret\n","\n","cleaned_data = clean_data(X, use_fe=False)\n"]},{"cell_type":"markdown","metadata":{"id":"Dm8fR8W4jUrW"},"source":["## 1.2 Model Creation, Hyperparameter Tuning, and Validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_validate\n","from sklearn.metrics import classification_report, roc_auc_score\n","from sklearn import metrics\n","import matplotlib.pyplot as plt\n","\n","def display_cv(classifier, data, truth):\n","    classifier.fit(data, truth)\n","    predicted = classifier.predict(data)\n","\n","    # We use cross_validate to perform K-fold cross validation for us.\n","    cv_results = cross_validate(classifier, data, truth, cv=5, scoring=\"f1_macro\")\n","    f1_score = np.mean(cv_results['test_score'])\n","    display(f\"results f1: {f1_score}\")\n","\n","    print(classification_report(truth, predicted))\n","    print(f\"auc: {roc_auc_score(truth, predicted)}\")\n","\n","    confusion_matrix = metrics.confusion_matrix(truth, predicted)\n","    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n","    cm_display.plot()\n","    plt.show()    "]},{"cell_type":"markdown","metadata":{},"source":["### Testing with a DecisionTreeClassifier "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn_genetic import GASearchCV\n","from sklearn_genetic.space import Continuous, Categorical, Integer\n","from sklearn_genetic.callbacks import DeltaThreshold\n","from sklearn_genetic.callbacks import ProgressBar\n","\n","pg_bar_callback = ProgressBar()\n","delta_callback = DeltaThreshold(threshold=0.0001, metric='fitness')\n","\n","DO_DTC_TUNING = False\n","\n","def run_hypertune_test_dt(data, truth):\n","    search_params_to_use = {\n","        'criterion': Categorical(choices=['gini', 'entropy', 'log_loss'], random_state=1),\n","        'max_depth': Integer(3, 8, 'uniform', random_state=1),\n","        'class_weight': Categorical(choices=['balanced', None], random_state=1),\n","        'min_samples_split': Continuous(0.5, 1.0, 'uniform', random_state=1),\n","        'min_samples_leaf': Integer(5, 15, 'uniform', random_state=1),\n","        # 'min_weight_fraction_leaf': Continuous(0.2, 0.5, 'uniform', random_state=1),\n","        # 'max_features': Continuous(0.5, 1.0, 'uniform', random_state=1),\n","        #'min_impurity_decrease': Continuous(0.0, 0.5, 'uniform', random_state=1),\n","    }\n","    \n","    test_model = DecisionTreeClassifier(random_state=1).fit(data, truth)\n","\n","    grid_search = GASearchCV(test_model, param_grid=search_params_to_use, n_jobs=-1, scoring=\"f1_macro\")\n","    grid_search.fit(data, truth, callbacks=[pg_bar_callback, delta_callback])\n","    display(grid_search.best_score_)\n","    best_params = grid_search.best_params_\n","    display(grid_search.best_params_)\n","\n","    test_model = DecisionTreeClassifier(random_state=1, **best_params).fit(data, truth)\n","    display_cv(test_model, data, truth)\n","    return best_params\n","\n","best_dt_params = {}\n","if DO_DTC_TUNING:\n","    best_dt_params = run_hypertune_test_dt(cleaned_data, y)\n","else:\n","    best_dt_params = {\n","        'class_weight': 'balanced',\n","        'max_depth': 6, \n","        'min_samples_leaf': 9,\n","        'ccp_alpha': 0\n","    }\n","    \n","\n","def get_dt_classifier(data, truth, parameters):\n","    print(f'-- DecisionTreeClassifier using: {parameters}')\n","    return DecisionTreeClassifier(random_state=1, **parameters).fit(data, truth)\n","\n","display(best_dt_params)\n","test_c = get_dt_classifier(cleaned_data, y, best_dt_params)\n","display_cv(test_c, cleaned_data, y)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Testing / Tuning with HistGradientBoostingClassifier "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import HistGradientBoostingClassifier\n","from sklearn_genetic import GASearchCV\n","from sklearn_genetic.space import Continuous, Categorical, Integer\n","from sklearn_genetic.callbacks import DeltaThreshold\n","\n","delta_callback = DeltaThreshold(threshold=0.0001, metric='fitness')\n","\n","DO_HGB_TUNING = False\n","\n","def run_hypertune_test_hgb(data, truth):\n","    search_params_to_use = {\n","        'learning_rate': Continuous(0.1, 1.0, 'uniform', random_state=1),\n","        'class_weight': Categorical(choices=['balanced', None], random_state=1),\n","        'max_leaf_nodes': Integer(20, 100, 'uniform', random_state=1),\n","        'max_depth': Integer(20, 100, 'uniform', random_state=1),\n","        'max_features': Continuous(0.4, 1.0, 'uniform', random_state=1),\n","        'max_iter': Integer(100, 300, 'uniform'),\n","        'l2_regularization': Continuous(0.0, 2.0, 'uniform', random_state=1),\n","    }\n","\n","    test_model = HistGradientBoostingClassifier(random_state=1).fit(data, truth)\n","    grid_search = GASearchCV(test_model, param_grid=search_params_to_use, n_jobs=-1, scoring=\"f1_macro\")\n","    grid_search.fit(data, truth, callbacks=[pg_bar_callback, delta_callback])\n","    display(grid_search.best_score_)\n","    best_params = grid_search.best_params_\n","    display(grid_search.best_params_)\n","\n","    test_model = HistGradientBoostingClassifier(random_state=1, **best_params).fit(data.to_numpy(), truth.to_numpy())\n","    display_cv(test_model, data, truth)\n","    return best_params\n","\n","best_hgb_params = {}\n","if DO_HGB_TUNING:\n","    best_hgb_params = run_hypertune_test_hgb(cleaned_data, y)\n","else:\n","    best_hgb_params = {\n","        'learning_rate': 0.2331298322064609,\n","        'class_weight': None,\n","        'max_leaf_nodes': 26,\n","        'max_depth': 90,\n","        'max_features': 0.48875322147097394,\n","        'max_iter': 119,\n","        'l2_regularization': 1.6625496693289223\n","    }\n","\n","def get_hgb_classifier(data, truth, params_to_use):\n","    print(f'-- HistGradientBoostingClassifier using: {params_to_use}')\n","    return HistGradientBoostingClassifier(random_state=1, **params_to_use).fit(data, truth)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Testing / Tuning with XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# do some testing...\n","DO_RUN_TUNING = False\n","\n","import xgboost as xgb\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","from sklearn_genetic.space import Continuous, Categorical, Integer\n","from sklearn_genetic.callbacks import DeltaThreshold\n","\n","delta_callback = DeltaThreshold(threshold=0.00005, metric='fitness')\n","def run_hypertune_test(data, truth):\n","\n","    # tuning process\n","    # 1. learning rate\n","    # 2. max_depth and min_child_weight\n","    # 3. gamma\n","    # 4. subsample and colsample_bytree\n","    # 5. then reduce the learning rate\n","    \n","    search_params_to_use = {\n","        'objective': Categorical(['binary:logitraw', 'binary:logistic'], random_state=1),\n","        'scale_pos_weight': Continuous(2.0, 4.5, 'uniform', random_state=1),\n","        'min_child_weight': Continuous(0.1, 3.0, 'uniform', random_state=1),\n","        'learning_rate': Continuous(0.08, 0.3, 'uniform', random_state=1),\n","        'max_depth': Integer(2, 5, 'uniform', random_state=1),\n","        'subsample': Continuous(0.9, 1.0, 'uniform', random_state=1),\n","        'colsample_bytree': Continuous(0.0, 1.0, 'uniform', random_state=1),\n","        'colsample_bylevel': Continuous(0.0, 1.0, 'uniform', random_state=1),\n","        'colsample_bynode': Continuous(0.0, 1.0, 'uniform', random_state=1),\n","        'lambda': Continuous(0.5, 1.5, 'uniform', random_state=1),\n","        'alpha': Continuous(0.0, 1.5, 'uniform', random_state=1),\n","        'gamma': Continuous(0.0, 1.5, 'uniform', random_state=1),\n","        'n_estimators': Integer(50, 500, 'uniform', random_state=1),\n","    }\n","\n","    test_model = xgb.XGBClassifier(random_state=1).fit(data.to_numpy(), truth.to_numpy())\n","    grid_search = GASearchCV(test_model, param_grid=search_params_to_use, n_jobs=-1, scoring=\"f1_macro\")    \n","    grid_search.fit(data, truth, callbacks=[pg_bar_callback, delta_callback])\n","    display(grid_search.best_score_)\n","    best_params = grid_search.best_params_\n","    display(grid_search.best_params_)\n","\n","    test_model = xgb.XGBClassifier(random_state=1, **best_params).fit(data.to_numpy(), truth.to_numpy())\n","    display_cv(test_model, data, truth)\n","    return best_params\n","\n","best_xgb_params = {}\n","if DO_RUN_TUNING:\n","    best_xgb_params = run_hypertune_test(cleaned_data, y)\n","else:\n","    best_xgb_params = {\n","        'objective': 'binary:logitraw',\n","        'scale_pos_weight': 3.2737397038037734,\n","        'min_child_weight': 1.5775380564123773,\n","        'learning_rate': 0.19208909393473206,\n","        'max_depth': 3,\n","        'subsample': 0.9509495881521509,\n","        'colsample_bytree': 0.5094958815215094,\n","        'colsample_bylevel': 0.9636708728449709,\n","        'colsample_bynode': 0.5094958815215094,\n","        'lambda': 1.0094958815215094,\n","        'alpha': 0.7642438222822641,\n","        'gamma': 0.7642438222822641,\n","        'n_estimators': 419\n","    }\n","\n","\n","def get_xgboost_classifier(data, truth, params_to_use = None):\n","    if params_to_use:\n","        final_params = params_to_use\n","    else:\n","        final_params = {\n","            # use this to help with the imbalanced targets\n","            'scale_pos_weight': training_data_pos_scale_weight,\n","            # it's a binary classifier - this method seems to work best\n","            'objective': 'binary:logitraw',\n","        }\n","\n","    print(f'-- XGBoost using: {final_params}')\n","\n","    return xgb.XGBClassifier(random_state=1, **final_params).fit(data, truth)\n","\n","display(best_xgb_params)\n","test_c = get_xgboost_classifier(cleaned_data, y, best_xgb_params)\n","display_cv(test_c, cleaned_data, y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSumAZUAo9O6"},"outputs":[],"source":["def run_xgboost(data, truth, params_to_use = None):\n","    clf = get_xgboost_classifier(data, truth, params_to_use)\n","    display_cv(clf, data, truth)\n","    return clf\n","\n","run_xgboost(cleaned_data, y)\n","run_xgboost(cleaned_data, y, params_to_use = best_xgb_params )\n","\n","# note - adding our current FE seems to do worse...\n"]},{"cell_type":"markdown","metadata":{},"source":["### Multiple classifiers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import VotingClassifier, StackingClassifier\n","\n","def get_voting_classifier(data, truth):\n","    xgboost_tuned = get_xgboost_classifier(data, truth, best_xgb_params)\n","    hgb_tuned = get_hgb_classifier(data, truth, best_hgb_params)\n","    dt_tuned = get_dt_classifier(data, truth, best_dt_params)\n","\n","    return VotingClassifier([\n","        # ('xgboost_base', xgboost_base),\n","        ('xgboost_tuned', xgboost_tuned),\n","        ('hgb_tuned', hgb_tuned),\n","        ('dt_tuned', dt_tuned),\n","    ], voting='soft').fit(data, truth)\n","\n","def run_voting_classifier(data, truth):\n","    clf = get_voting_classifier(data, truth)\n","    display_cv(clf, data, truth)\n","    return clf\n","\n","def get_stacking_classifer(data, truth):\n","    xgboost_tuned = get_xgboost_classifier(data, truth, best_xgb_params)\n","    hgb_tuned = get_hgb_classifier(data, truth, best_hgb_params)\n","\n","    return StackingClassifier([\n","        ('xgboost_tuned', xgboost_tuned),\n","        ('hgb_tuned', hgb_tuned),\n","    ], cv=5, n_jobs=-1).fit(data, truth)\n","\n","def run_stacking_classifier(data, truth):\n","    clf = get_stacking_classifer(data, truth)\n","    display_cv(clf, data, truth)\n","    return clf\n","\n","clf = run_voting_classifier(cleaned_data, y)\n","# clf = run_stacking_classifier(cleaned_data, y)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Let's check our features with our final model - and see what we might want to tune out..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn_genetic.genetic_search import GAFeatureSelectionCV\n","\n","FLAG_FIND_DROP_FEATURES = False\n","\n","best_features_to_drop = []\n","\n","def get_tuples_features_to_drop(data, truth):\n","    final_classifier = get_voting_classifier(data, truth)\n","\n","    evolved_estimator = GAFeatureSelectionCV(\n","        estimator=final_classifier,\n","        cv=5,\n","        scoring=\"f1_macro\",\n","        population_size=30,\n","        generations=20,\n","        n_jobs=-1,\n","        verbose=True,\n","        keep_top_k=2,\n","        elitism=True,\n","    )\n","\n","    input_features = list(data.columns)\n","    evolved_estimator.fit(data, truth)\n","    best_features = evolved_estimator.best_features_\n","    ret = zip(input_features, best_features)\n","    display(tuple(ret))\n","    \n","    from sklearn_genetic.plots import plot_fitness_evolution\n","    plot_fitness_evolution(evolved_estimator)\n","    plt.show()\n","\n","    return ret\n","\n","def get_list_features_to_drop(data, truth):\n","    tuples_to_drop = get_tuples_features_to_drop(data, truth)\n","    features_to_drop = []\n","    for feature in tuples_to_drop:\n","        if not feature[1]:\n","            features_to_drop.append(feature[0])\n","    display(f'Dropping: {features_to_drop}')\n","    return features_to_drop\n","\n","if FLAG_FIND_DROP_FEATURES:\n","    best_features_to_drop = get_list_features_to_drop(cleaned_data, y)\n","\n","cleaned_data = cleaned_data.drop(best_features_to_drop, axis=1)\n","\n","clf = run_voting_classifier(cleaned_data, y)\n","# clf = run_stacking_classifier(cleaned_data, y)\n","# clf = run_xgboost(cleaned_data, y, best_xgb_params)\n"]},{"cell_type":"markdown","metadata":{},"source":["### The above test uses FE on the cleaned data as well - \n","Compare the results above with the results before the dropping of features..."]},{"cell_type":"markdown","metadata":{"id":"ihVtYBWg1NM6"},"source":["## 1.4: Create Predictions for Competition Data\n","\n","Once we are happy with the estimated performance of our model, we can move on to the final step.\n","\n","First, we train our model one last time, using all available training data (unlike CV, which always uses a subset). This final training will give our model the best chance as the highest performance.\n","\n","Then, we must load in the (unlabeled) competition data from the cloud and use our model to generate predictions for each instance in that data. We will then output those predictions to a CSV file. We will then send that file to Steve, and he can then tell us how well we did (because he knows the right answers!)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74lBpHWfe5h1"},"outputs":[],"source":["# Our model's \"final form\"\n","\n","clf = clf.fit(cleaned_data, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pu2xugQj1Mci"},"outputs":[],"source":["X_comp = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1SmFBoNh7segI1Ky92mfeIe6TpscclMwQ\")\n","\n","# Importantly, we need to perform the same cleaning/transformation steps\n","# on this competition data as you did the training data. Otherwise, we will\n","# get an error and/or unexpected results.\n","\n","X_comp = clean_data(X_comp, use_fe=False)\n","X_comp = X_comp.drop(best_features_to_drop, axis=1)\n","\n","# Use your model to make predictions\n","pred_comp = clf.predict(X_comp)\n","\n","my_submission = pd.DataFrame({'predicted': pred_comp})\n","\n","pred_data_pos_scale_weight = get_target_skew_rate(pd.DataFrame(pred_comp, columns=[target_feature]))\n","\n","display(f'training target skew: {training_data_pos_scale_weight}')\n","display(f'predicted target skew: {pred_data_pos_scale_weight}')\n","display(f'difference: {abs(training_data_pos_scale_weight - pred_data_pos_scale_weight)}')\n","\n","# Let's take a peak at the results (as a sanity check)\n","display(my_submission.head(10))\n","\n","# You could use any filename.\n","my_submission.to_csv('my_submission.csv', index=False)\n","\n","# You can now download the above file from Colab (see menu on the left)"]},{"cell_type":"markdown","metadata":{"id":"1DF6Z8-baL9K"},"source":["# Model 2 (Your idea Here!)\n","\n","Here, you can do all the above, but try different ideas:\n","\n","- Different ML algorithms (e.g., RandomForestClassifier, LGBM, NN)\n","- Different data cleaning steps (Ordinal encoding, One Hot Encoding, etc.)\n","- Hyperparameter tuning (using, e.g., GridSearchCV or RandomizedSearchCV)\n","- Ensembles\n","- .... anything you can think of!\n","\n","\n","Steve's GitHub page is a great place for ideas:\n","\n","https://github.com/stepthom/869_course"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"heJMDH4KaN9G"},"outputs":[],"source":["# TODO: Win the competition here!"]},{"cell_type":"markdown","metadata":{"id":"qquC4XuEiyA5"},"source":["# Model 3 (Your next idea here!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKrudgbEiyqw"},"outputs":[],"source":["# TODO: Win the competition here, too!"]}],"metadata":{"colab":{"provenance":[{"file_id":"1Qdzbejqt4ypSvL-h8WFMmQFEFZB2GKId","timestamp":1716377771632},{"file_id":"1ER4NDAWpuwaZ43Dn9vYxd_vapYEDeczF","timestamp":1652728119372},{"file_id":"1InS6-7UZYc8qRPKGhEBkGv8nMQ3dl3Qq","timestamp":1631545645387},{"file_id":"1Pp88aji8w35O-WJbctcsHfinGqUhoT1c","timestamp":1622488006439},{"file_id":"1PNmd4hys3w1ZYkMSzqYL7fVV3v72Chxv","timestamp":1614187967149},{"file_id":"1Nv7GmhY_xL7txEJ0nNKNP1WmMiXEHA5U","timestamp":1613739709908},{"file_id":"https://github.com/stepthom/NLP_course/blob/main/document_classification/kiva_classification_simple.ipynb","timestamp":1613482590973}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
