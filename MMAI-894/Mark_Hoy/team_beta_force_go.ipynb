{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "import shutil\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Architecture\n",
    "\n",
    "The architecture of the system takes an image and a question as inputs and outputs (via a classifier) an answer. This is designed as:\n",
    "\n",
    "1. a CNN to encode the image into an embedding vector representation (size: 768)\n",
    "2. a LTSM encoder to encode the question into an embedding vector representation (size: 384)\n",
    "3. the two vectors above to create the input into the classifier (total input size: 1152) with a one-hot encoded vector of the unique class labels from the training dataset as the outputs (size: 2521)\n",
    "\n",
    "The image encoding is done via a pretrained model from HuggingFace : google/vit-base-patch16-224-in21k - (https://huggingface.co/google/vit-base-patch16-224-in21k)\n",
    "\n",
    "The question encoding is done in one pass via the pretrained sentence-transformers/all-MiniLM-L6-v2 model - (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "\n",
    "The final classification layers are:\n",
    "* Input layer : input size (1152,)\n",
    "* Dense layer : 1024 nodes\n",
    "* Dense layer : 1024 nodes\n",
    "* Output layer : 2521 nodes, with softmax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"./data\"\n",
    "\n",
    "# the CSV training data - preprocessed from before via the preprocess_annotations.py\n",
    "TRAINING_DATA = os.path.join(DATA_DIRECTORY, \"combined_training.csv\")\n",
    "VALIDATION_DATA = os.path.join(DATA_DIRECTORY, \"combined_validation.csv\")\n",
    "\n",
    "# our images directories\n",
    "TRAINING_IMAGES = os.path.join(DATA_DIRECTORY, \"scene_img_abstract_v002_train2015\")\n",
    "VALIDATION_IMAGES = os.path.join(DATA_DIRECTORY, \"scene_img_abstract_v002_val2015\")\n",
    "\n",
    "# saving our TF datasets so we don't have to rebuild everytime\n",
    "TRAINING_ENCODED_DATASET = os.path.join(DATA_DIRECTORY, \"training_encoded_dataset\")\n",
    "VALIDATION_ENCODED_DATASET = os.path.join(DATA_DIRECTORY, \"validation_encoded_dataset\")\n",
    "\n",
    "TRAINING_PREENCODED_QUESTIONS = os.path.join(DATA_DIRECTORY, \"training_encoded_questions.pkl\")\n",
    "VALIDATION_PREENCODED_QUESTIONS = os.path.join(DATA_DIRECTORY, \"validation_encoded_questions.pkl\")\n",
    "\n",
    "TRAINING_PREENCODED_IMAGES = os.path.join(DATA_DIRECTORY, \"training_encoded_images.pkl\")\n",
    "VALIDATION_PREENCODED_IMAGES = os.path.join(DATA_DIRECTORY, \"validation_encoded_images.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/jinaai/jina-embeddings-v3\n",
    "from transformers import AutoModel\n",
    "embeddings_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "# this is the specific embeddings size for the all-MiniLM-L6-v2 model\n",
    "embeddings_vector_size = 768\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/vit\n",
    "# https://huggingface.co/google/vit-base-patch16-224-in21k\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "image_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "pooling_layer_size = 768\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def encode_image(path_to_image):\n",
    "    image = Image.open(path_to_image).convert(\"RGB\").resize((224, 224))\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\", do_rescale=True)\n",
    "    outputs = image_model(**inputs)\n",
    "    encoded_image = outputs.pooler_output.detach().numpy()[0]\n",
    "    return encoded_image.tolist()\n",
    "\n",
    "def encode_question(question_text):\n",
    "    encoded = embeddings_model.encode(question_text.lower(), truncate_dim=embeddings_vector_size)\n",
    "    return encoded.tolist()\n",
    "\n",
    "def pairwise_multiplication(image, question):\n",
    "    ret = []\n",
    "    for i in range(len(image)):\n",
    "        ret.append(image[i] * question[i])\n",
    "    return ret\n",
    "\n",
    "def get_encoded_image_question(path_to_image, question_text, pre_encoded_images = None, pre_encoded_questions = None):\n",
    "    if pre_encoded_images:\n",
    "        encoded_image = pre_encoded_images[path_to_image]\n",
    "    else:\n",
    "        encoded_image = encode_image(path_to_image)\n",
    "        \n",
    "    if pre_encoded_questions:\n",
    "        encoded_question = pre_encoded_questions[question_text]\n",
    "    else:\n",
    "        encoded_question = encode_question(question_text)\n",
    "    return pairwise_multiplication(encoded_image, encoded_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_questions(array_of_questions):\n",
    "    ret = {}\n",
    "    for question in tqdm(array_of_questions):\n",
    "        ret[question] = encode_question(question)\n",
    "    return ret\n",
    "\n",
    "def encode_images(array_of_images):\n",
    "    ret = {}\n",
    "    for image in tqdm(array_of_images):\n",
    "        ret[image] = encode_image(image)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique answers: 2521\n"
     ]
    }
   ],
   "source": [
    "# load our training dataset\n",
    "\n",
    "cnn_training_df = pd.read_csv(TRAINING_DATA)\n",
    "cnn_training_df.drop(cnn_training_df.columns.difference(['image_filename', 'question', 'multiple_choice_answer']), axis=1, inplace=True)\n",
    "cnn_training_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# also extract all the unique answers from it\n",
    "unique_answers = [x.lower() for x in cnn_training_df.multiple_choice_answer.unique().tolist()]\n",
    "unique_answers_len = len(unique_answers)\n",
    "\n",
    "unique_answers_map = {}\n",
    "for index, answer in enumerate(unique_answers):\n",
    "    unique_answers_map[answer] = index\n",
    "\n",
    "print(f'Total unique answers: {unique_answers_len}')\n",
    "\n",
    "# helper function to create a 1-hot encoded vector of an answer\n",
    "def one_hot_answer_encode(answer):\n",
    "    ret = [0] * unique_answers_len\n",
    "    lower_answer = answer.lower()\n",
    "    if lower_answer in unique_answers_map:\n",
    "        answer_index = unique_answers_map[lower_answer]\n",
    "        ret[answer_index] = 1\n",
    "        return ret\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved pre-encoding questions\n",
      "loading saved images\n",
      "building training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59983/59983 [00:05<00:00, 10411.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving dataset\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "# comment out the line above to rebuild the training dataset\n",
    "\n",
    "if os.path.exists(TRAINING_PREENCODED_QUESTIONS):\n",
    "    print('loading saved pre-encoding questions')\n",
    "    with open(TRAINING_PREENCODED_QUESTIONS, 'rb') as infile:\n",
    "        pre_encoded_questions = pickle.load(infile)\n",
    "else:\n",
    "    print('pre-encoding questions')\n",
    "    unique_questions = cnn_training_df.question.unique().tolist()\n",
    "    pre_encoded_questions = encode_questions(unique_questions)\n",
    "    with open(TRAINING_PREENCODED_QUESTIONS, 'wb') as outfile:\n",
    "        pickle.dump(pre_encoded_questions, outfile)\n",
    "\n",
    "if os.path.exists(TRAINING_PREENCODED_IMAGES):\n",
    "    print('loading saved images')\n",
    "    with open(TRAINING_PREENCODED_IMAGES, 'rb') as infile:\n",
    "        pre_encoded_images = pickle.load(infile)\n",
    "else:\n",
    "    print('pre-encoding images')\n",
    "    unique_images = cnn_training_df.image_filename.unique().tolist()\n",
    "    unique_images = [os.path.join(TRAINING_IMAGES, x) for x in unique_images]\n",
    "    pre_encoded_images = encode_images(unique_images)\n",
    "    with open(TRAINING_PREENCODED_IMAGES, 'wb') as outfile:\n",
    "        pickle.dump(pre_encoded_images, outfile)\n",
    "\n",
    "# build the training dataset\n",
    "print('building training data...')\n",
    "data_embeddings = []\n",
    "data_outputs = []\n",
    "for index, row in tqdm(cnn_training_df.iterrows(), total=cnn_training_df.shape[0]):\n",
    "    this_image_filename = row[\"image_filename\"]\n",
    "    this_question = row[\"question\"]\n",
    "    this_answer = row[\"multiple_choice_answer\"]\n",
    "\n",
    "    one_hot_answer = one_hot_answer_encode(this_answer)\n",
    "    if not one_hot_answer:\n",
    "        continue\n",
    "\n",
    "    embedding_input = get_encoded_image_question(\n",
    "        os.path.join(TRAINING_IMAGES, this_image_filename),\n",
    "        this_question,\n",
    "        pre_encoded_images=pre_encoded_images,\n",
    "        pre_encoded_questions=pre_encoded_questions\n",
    "    )\n",
    "\n",
    "    data_embeddings.append(embedding_input)\n",
    "    data_outputs.append(one_hot_answer)\n",
    "\n",
    "    # if len(data_outputs) == 20:\n",
    "    #     break\n",
    "\n",
    "cnn_training_dataset = tf.data.Dataset.from_tensor_slices(([data_embeddings], [data_outputs]))\n",
    "print(\"saving dataset\")\n",
    "if os.path.exists(TRAINING_ENCODED_DATASET):\n",
    "    shutil.rmtree(TRAINING_ENCODED_DATASET)\n",
    "\n",
    "os.makedirs(TRAINING_ENCODED_DATASET)\n",
    "\n",
    "cnn_training_dataset.save(TRAINING_ENCODED_DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our validation dataset\n",
    "cnn_validation_df = pd.read_csv(VALIDATION_DATA)\n",
    "cnn_validation_df.drop(cnn_validation_df.columns.difference(['image_filename', 'question', 'multiple_choice_answer']), axis=1, inplace=True)\n",
    "cnn_validation_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoding questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19199/19199 [40:04<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoding images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [18:17<00:00,  9.11it/s]\n",
      "100%|██████████| 29990/29990 [00:02<00:00, 13885.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving dataset\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "# comment out the line above to rebuild the validation dataset\n",
    "\n",
    "if os.path.exists(VALIDATION_PREENCODED_QUESTIONS):\n",
    "    print('loading saved pre-encoding questions')\n",
    "    with open(VALIDATION_PREENCODED_QUESTIONS, 'rb') as infile:\n",
    "        pre_encoded_questions = pickle.load(infile)\n",
    "else:\n",
    "    print('pre-encoding questions')\n",
    "    unique_questions = cnn_validation_df.question.unique().tolist()\n",
    "    pre_encoded_questions = encode_questions(unique_questions)\n",
    "    with open(VALIDATION_PREENCODED_QUESTIONS, 'wb') as outfile:\n",
    "        pickle.dump(pre_encoded_questions, outfile)\n",
    "\n",
    "if os.path.exists(VALIDATION_PREENCODED_IMAGES):\n",
    "    print('loading saved images')\n",
    "    with open(VALIDATION_PREENCODED_IMAGES, 'rb') as infile:\n",
    "        pre_encoded_images = pickle.load(infile)\n",
    "else:\n",
    "    print('pre-encoding images')\n",
    "    unique_images = cnn_validation_df.image_filename.unique().tolist()\n",
    "    unique_images = [os.path.join(VALIDATION_IMAGES, x) for x in unique_images]\n",
    "    pre_encoded_images = encode_images(unique_images)\n",
    "    with open(VALIDATION_PREENCODED_IMAGES, 'wb') as outfile:\n",
    "        pickle.dump(pre_encoded_images, outfile)\n",
    "\n",
    "data_embeddings = []\n",
    "data_outputs = []\n",
    "for index, row in tqdm(cnn_validation_df.iterrows(), total=cnn_validation_df.shape[0]):\n",
    "    this_image_filename = row[\"image_filename\"]\n",
    "    this_question = row[\"question\"]\n",
    "    this_answer = row[\"multiple_choice_answer\"]\n",
    "\n",
    "    one_hot_answer = one_hot_answer_encode(this_answer)\n",
    "    if not one_hot_answer:\n",
    "        continue\n",
    "\n",
    "    embedding_input = get_encoded_image_question(\n",
    "        os.path.join(VALIDATION_IMAGES, this_image_filename),\n",
    "        this_question,\n",
    "        pre_encoded_images=pre_encoded_images,\n",
    "        pre_encoded_questions=pre_encoded_questions\n",
    "    )\n",
    "\n",
    "    data_embeddings.append(embedding_input)\n",
    "    data_outputs.append(one_hot_answer)\n",
    "\n",
    "    # if len(data_outputs) == 20:\n",
    "    #     break\n",
    "\n",
    "cnn_validation_dataset = tf.data.Dataset.from_tensor_slices(([data_embeddings], [data_outputs]))\n",
    "print(\"saving dataset\")\n",
    "if os.path.exists(VALIDATION_ENCODED_DATASET):\n",
    "    shutil.rmtree(VALIDATION_ENCODED_DATASET)\n",
    "\n",
    "os.makedirs(VALIDATION_ENCODED_DATASET)\n",
    "\n",
    "cnn_validation_dataset.save(VALIDATION_ENCODED_DATASET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload our datasets\n",
    "training_dataset = tf.data.Dataset.load(TRAINING_ENCODED_DATASET)\n",
    "validation_dataset = tf.data.Dataset.load(VALIDATION_ENCODED_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 1024)              787456    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2521)              2584025   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4421081 (16.87 MB)\n",
      "Trainable params: 4421081 (16.87 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model creation\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "input_vector_size = embeddings_vector_size # 768\n",
    "output_vector_size = unique_answers_len # 2521 as of this last run\n",
    "\n",
    "qa_model = Sequential()\n",
    "\n",
    "# our combined vector is the input\n",
    "qa_model.add(Input(shape=(input_vector_size,)))\n",
    "\n",
    "# two densely connected layers\n",
    "qa_model.add(Dense(1024,activation='relu'))\n",
    "qa_model.add(Dense(1024,activation='relu'))\n",
    "\n",
    "# and our output layer\n",
    "qa_model.add(Dense(unique_answers_len, activation=\"softmax\"))\n",
    "\n",
    "qa_model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"acc\", \"AUC\"]\n",
    ")\n",
    "qa_model.build(input_shape=(input_vector_size,))\n",
    "\n",
    "display(qa_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 6s 6s/step - loss: 7.8301 - acc: 4.6680e-04 - auc: 0.0555 - val_loss: 7.7997 - val_acc: 0.2494 - val_auc: 0.1135\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 6s 6s/step - loss: 7.8007 - acc: 0.2386 - auc: 0.0555 - val_loss: 7.7567 - val_acc: 0.2494 - val_auc: 0.1135\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 6s 6s/step - loss: 7.7591 - acc: 0.2386 - auc: 0.0555 - val_loss: 7.6888 - val_acc: 0.2494 - val_auc: 0.1135\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 6s 6s/step - loss: 7.6933 - acc: 0.2386 - auc: 0.0555 - val_loss: 7.5835 - val_acc: 0.2494 - val_auc: 0.1135\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 6s 6s/step - loss: 7.5911 - acc: 0.2386 - auc: 0.0555 - val_loss: 7.4269 - val_acc: 0.2494 - val_auc: 0.1135\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 6s 6s/step - loss: 7.4391 - acc: 0.2386 - auc: 0.0555 - val_loss: 7.2040 - val_acc: 0.2494 - val_auc: 0.1135\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 7.2227 - acc: 0.2386 - auc: 0.0555 - val_loss: 6.8995 - val_acc: 0.2494 - val_auc: 0.1135\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 6.9272 - acc: 0.2386 - auc: 0.0555 - val_loss: 6.5005 - val_acc: 0.2494 - val_auc: 0.1155\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 6.5403 - acc: 0.2386 - auc: 0.0564 - val_loss: 6.0029 - val_acc: 0.2494 - val_auc: 0.1462\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 6.0583 - acc: 0.2386 - auc: 0.0709 - val_loss: 5.4240 - val_acc: 0.2494 - val_auc: 0.1633\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 5.4997 - acc: 0.2386 - auc: 0.0792 - val_loss: 4.8276 - val_acc: 0.2494 - val_auc: 0.1739\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 4.9289 - acc: 0.2386 - auc: 0.0848 - val_loss: 4.3474 - val_acc: 0.2494 - val_auc: 0.1993\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 4.4802 - acc: 0.2386 - auc: 0.0997 - val_loss: 4.1567 - val_acc: 0.2494 - val_auc: 0.1985\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 4.3255 - acc: 0.2386 - auc: 0.0989 - val_loss: 4.2404 - val_acc: 0.2494 - val_auc: 0.1903\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 8s 8s/step - loss: 4.4429 - acc: 0.2386 - auc: 0.0938 - val_loss: 4.3287 - val_acc: 0.2494 - val_auc: 0.1890\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 9s 9s/step - loss: 4.5553 - acc: 0.2386 - auc: 0.0930 - val_loss: 4.3607 - val_acc: 0.2494 - val_auc: 0.1902\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 10s 10s/step - loss: 4.6016 - acc: 0.2386 - auc: 0.0936 - val_loss: 4.3463 - val_acc: 0.1720 - val_auc: 0.1921\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 11s 11s/step - loss: 4.5925 - acc: 0.1691 - auc: 0.0948 - val_loss: 4.2920 - val_acc: 0.1720 - val_auc: 0.1942\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 12s 12s/step - loss: 4.5367 - acc: 0.1691 - auc: 0.0962 - val_loss: 4.2222 - val_acc: 0.1720 - val_auc: 0.1962\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 12s 12s/step - loss: 4.4608 - acc: 0.1691 - auc: 0.0975 - val_loss: 4.1554 - val_acc: 0.1720 - val_auc: 0.1990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [7.830138683319092,\n",
       "  7.80070686340332,\n",
       "  7.759055137634277,\n",
       "  7.693273544311523,\n",
       "  7.591084003448486,\n",
       "  7.439083576202393,\n",
       "  7.222714900970459,\n",
       "  6.927210807800293,\n",
       "  6.5402750968933105,\n",
       "  6.0583295822143555,\n",
       "  5.499658107757568,\n",
       "  4.928905963897705,\n",
       "  4.480183124542236,\n",
       "  4.325539588928223,\n",
       "  4.442935943603516,\n",
       "  4.555269241333008,\n",
       "  4.601552963256836,\n",
       "  4.5925211906433105,\n",
       "  4.536729335784912,\n",
       "  4.460849285125732],\n",
       " 'acc': [0.0004667989269364625,\n",
       "  0.2385842651128769,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.23856759071350098,\n",
       "  0.1690979152917862,\n",
       "  0.1690979152917862,\n",
       "  0.1690979152917862],\n",
       " 'auc': [0.055495940148830414,\n",
       "  0.055495940148830414,\n",
       "  0.055495940148830414,\n",
       "  0.055495940148830414,\n",
       "  0.055495940148830414,\n",
       "  0.055495940148830414,\n",
       "  0.055495940148830414,\n",
       "  0.055495940148830414,\n",
       "  0.05644255504012108,\n",
       "  0.07085112482309341,\n",
       "  0.07922770082950592,\n",
       "  0.08476059138774872,\n",
       "  0.09970825910568237,\n",
       "  0.098938949406147,\n",
       "  0.09380800276994705,\n",
       "  0.09295626729726791,\n",
       "  0.09360337257385254,\n",
       "  0.09481441229581833,\n",
       "  0.0961546078324318,\n",
       "  0.09746496379375458],\n",
       " 'val_loss': [7.7996697425842285,\n",
       "  7.756671905517578,\n",
       "  7.688821792602539,\n",
       "  7.583491802215576,\n",
       "  7.426871299743652,\n",
       "  7.203951358795166,\n",
       "  6.899452209472656,\n",
       "  6.500487804412842,\n",
       "  6.002854824066162,\n",
       "  5.423990249633789,\n",
       "  4.827619552612305,\n",
       "  4.347439765930176,\n",
       "  4.156731605529785,\n",
       "  4.240434646606445,\n",
       "  4.328667163848877,\n",
       "  4.360722541809082,\n",
       "  4.3463053703308105,\n",
       "  4.292045593261719,\n",
       "  4.222235679626465,\n",
       "  4.155449867248535],\n",
       " 'val_acc': [0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.24939492344856262,\n",
       "  0.17197886109352112,\n",
       "  0.17197886109352112,\n",
       "  0.17197886109352112,\n",
       "  0.17197886109352112],\n",
       " 'val_auc': [0.11347580701112747,\n",
       "  0.11347580701112747,\n",
       "  0.11347580701112747,\n",
       "  0.11347580701112747,\n",
       "  0.11347580701112747,\n",
       "  0.11347580701112747,\n",
       "  0.11347580701112747,\n",
       "  0.1154719740152359,\n",
       "  0.14618860185146332,\n",
       "  0.16334004700183868,\n",
       "  0.17394356429576874,\n",
       "  0.1992972046136856,\n",
       "  0.198481023311615,\n",
       "  0.1902550607919693,\n",
       "  0.18903160095214844,\n",
       "  0.19015732407569885,\n",
       "  0.19208979606628418,\n",
       "  0.1942179799079895,\n",
       "  0.19616447389125824,\n",
       "  0.19895093142986298]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model training\n",
    "\n",
    "fit_history = qa_model.fit(\n",
    "    x = training_dataset,\n",
    "    batch_size=64,\n",
    "    epochs = 20,\n",
    "    validation_data=validation_dataset,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "display(fit_history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is the dog asleep? (image: ./data/scene_img_abstract_v002_val2015/abstract_v002_val2015_000000027578.png)\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "The answer found is: no (output_value: 0.26438599824905396)\n"
     ]
    }
   ],
   "source": [
    "def find_max_value_index(predictions):\n",
    "    max_index = 0\n",
    "    max_value = -99999999.99\n",
    "    for index, value in enumerate(predictions):\n",
    "        if value > max_value:\n",
    "            max_index = index\n",
    "            max_value = value\n",
    "    return max_index, max_value\n",
    "\n",
    "def answer_question(image_filepath, question_text):\n",
    "    embedding_input = get_encoded_image_question(image_filepath, question_text)\n",
    "    embedding_input = [embedding_input]\n",
    "    ds = tf.data.Dataset.from_tensor_slices([embedding_input])\n",
    "    predicted = qa_model.predict(ds)\n",
    "    predicted_as_list = predicted.tolist()[0]\n",
    "    return find_max_value_index(predicted_as_list)\n",
    "\n",
    "def test_qa(image_filepath, question_text):\n",
    "    print(f'Question: {question_text} (image: {image_filepath})')\n",
    "    answer_index, answer_prob_value = answer_question(image_filepath, question_text)\n",
    "    answer = unique_answers[answer_index]\n",
    "    print(f'The answer found is: {answer} (output_value: {answer_prob_value})')\n",
    "\n",
    "def get_image_path(filename):\n",
    "    return os.path.join(VALIDATION_IMAGES, filename)\n",
    "\n",
    "# testing\n",
    "testing_data = [\n",
    "    { 'question': 'Is the dog asleep?', \"image\": get_image_path('abstract_v002_val2015_000000027578.png') }\n",
    "]\n",
    "\n",
    "for test in testing_data:\n",
    "    test_qa(test['image'], test['question'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
